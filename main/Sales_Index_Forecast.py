# -*- coding: utf-8 -*-
"""DIC_Phase_3.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/16oU3uJNWcgaMtMpkXH1bTKMX2IQhRrJw
"""

from pyspark.sql import SparkSession
from pyspark.sql import functions as F
from pyspark import SparkContext, SparkConf

spark = SparkSession.builder.appName("FiservData").getOrCreate()

data = spark.read.csv('Fiserv_data_2019-01-01_to_2024-01-01.csv', header=True, inferSchema=True)

data.printSchema()

num_rows = data.count()
num_columns = len(data.columns)
print(f"Rows: {num_rows}, Columns: {num_columns}")

data.show(5)

null_counts = data.select([F.sum(F.col(c).isNull().cast("int")).alias(c) for c in data.columns])

null_counts.show()

"""Data Cleaning:

1. Handling Missing Values:
"""

#Filling the missing values by the mean of the column for large number of null values
spark = SparkSession.builder.appName("DataCleaning").getOrCreate()

sc = spark.sparkContext

sy_mean = data.select(F.mean('Sales YOY % - SA')).collect()[0][0]
data = data.fillna({'Sales YOY % - SA': sy_mean})

tm_mean = data.select(F.mean('Transaction MOM % - SA')).collect()[0][0]
data = data.fillna({'Transaction MOM % - SA': tm_mean})

ty_mean = data.select(F.mean('Transaction YOY %  - SA')).collect()[0][0]
data = data.fillna({'Transaction YOY %  - SA': ty_mean})

syn_mean = data.select(F.mean('Sales YOY % - NSA')).collect()[0][0]
data = data.fillna({'Sales YOY % - NSA': syn_mean})

tyn_mean = data.select(F.mean('Transaction YOY % - NSA')).collect()[0][0]
data = data.fillna({'Transaction YOY % - NSA': tyn_mean})

#Dropping the missing rows of rest of null values
data = data.dropna(subset=['Sales Index - SA'])
data = data.dropna(subset=['Transactional Index - SA'])
data = data.dropna(subset=['Sales MOM % - SA'])
data = data.dropna(subset=['Sales MOM % - NSA'])
data = data.dropna(subset=['Transaction MOM % - NSA'])

null_counts = data.select([F.sum(F.col(c).isNull().cast("int")).alias(c) for c in data.columns])

null_counts.show()

"""2. Date Component Extraction:

"""

#Converting the 'Period' column from string format (YYYYMMDD) to a datetime object for easier date manipulation and analysis.
data = data.withColumn('Period', F.to_date('Period', 'yyyyMMdd'))

data = data.withColumn('Year', F.year('Period'))
data = data.withColumn('Month', F.month('Period'))
data = data.withColumn('Day', F.dayofmonth('Period'))

data.select('Period', 'Year', 'Month', 'Day').show(5)

"""3. Feature Elimination:"""

# Dropping the 'Day' and 'Period' columns
data = data.drop('Day', 'Period')

data.show(5)

"""4. Column Renaming:"""

data.select('Geo').distinct().show()

# Renaming the 'Geo' column to 'State'
data = data.withColumnRenamed('Geo', 'State')

print(data.columns)

unique_states = data.select('State').distinct().rdd.flatMap(lambda x: x).collect()

for state in unique_states:
    print(state)

Region = [
    'Atlanta-Sandy Springs-Alpharetta GA',
    'Boston-Cambridge-Newton MA-NH',
    'Chicago-Naperville-Elgin IL-IN-WI',
    'Dallas-Fort Worth-Arlington TX',
    'Los Angeles-Long Beach-Anaheim CA',
    'Miami-Fort Lauderdale-Pompano Beach FL',
    'New York-Newark-Jersey City NY-NJ-PA',
    'Philadelphia-Camden-Wilmington PA-NJ-DE-MD',
    'San Francisco-Oakland-Berkeley CA',
    'Washington-Arlington-Alexandria DC-VA-MD-WV', 'US'
]

filtered_data = data.filter(data['State'].isin(Region))
Region_counts = filtered_data.groupBy('State').count()

Region_counts.show()

filtered_data = data.filter(~data['State'].isin(Region))

state_counts = filtered_data.groupBy('State').count()

state_counts.show()

for col in data.columns:
    new_col = col.strip()
    new_col = new_col.replace(' - ', '-')
    new_col = new_col.replace(' % ', '%')
    data = data.withColumnRenamed(col, new_col)

print(data.columns)

unique_subsectors = data.select('Sub-Sector Name').distinct()

unique_subsectors.show(truncate=False)

data = data[data['Sub-Sector Name'] != 'All']

data = data.withColumn(
    'Sector Name',
    F.when(
        data['Sector Name'] == 'Wholesale Trade (Motor Vehicle and Motor Vehicle Parts, Metal and Mineral, Household Appliances and Electrical and Electronic Goods, Durable Goods)',
        'Wholesale Trade'
    ).otherwise(data['Sector Name'])
)

data = data.withColumn(
    'Sector Name',
    F.when(
        data['Sector Name'] == 'Administrative and Support and Waste Management and Remediation Services',
        'Administrative and Environmental Services'
    ).otherwise(data['Sector Name'])
)

data.select('Sector Name').distinct().show(truncate=False)

"""5. Feature Engineering:"""

# Creating new columns by averaging the respective column's values
data = data.withColumn('Sales Index',
                       (data['Sales Index-NSA'] + data['Sales Index-SA']) / 2)

data = data.withColumn('Transactional Index',
                       (data['Transactional Index-NSA'] + data['Transactional Index-SA']) / 2)

data = data.withColumn('Transaction MOM %',
                       (data['Transaction MOM %-NSA'] + data['Transaction MOM %-SA']) / 2)

data = data.withColumn('Transaction YOY %',
                       (data['Transaction YOY %-NSA'] + data['Transaction YOY%-SA']) / 2)

data = data.withColumn('Sales MOM %',
                       (data['Sales MOM %-NSA'] + data['Sales MOM %-SA']) / 2)

data = data.withColumn('Sales YOY %',
                       (data['Sales YOY %-NSA'] + data['Sales YOY %-SA']) / 2)

# Dropping the original columns used for the calculation
data = data.drop('Sales Index-NSA', 'Sales Index-SA',
                 'Transactional Index-NSA', 'Transactional Index-SA',
                 'Transaction MOM %-NSA', 'Transaction MOM %-SA',
                 'Transaction YOY %-NSA', 'Transaction YOY%-SA',
                 'Sales MOM %-NSA', 'Sales MOM %-SA',
                 'Sales YOY %-NSA', 'Sales YOY %-SA')


data.show(5)

"""6. Removing Outliers:"""

from pyspark.sql.functions import col

#removing the outliers using inter quartile method
Q1, Q3 = data.approxQuantile("Sales YOY %", [0.25, 0.75], 0.01)
IQR = Q3 - Q1

lower_bound = Q1 - 1.5 * IQR
upper_bound = Q3 + 1.5 * IQR

data = data.filter((col("Sales YOY %") >= lower_bound) & (col("Sales YOY %") <= upper_bound))

Q1_y, Q3_y = data.approxQuantile("Transaction YOY %", [0.25, 0.75], 0.01)
IQR_y = Q3_y - Q1_y

lower_bound_y = Q1_y - 1.5 * IQR_y
upper_bound_y = Q3_y + 1.5 * IQR_y

data = data.filter((col("Transaction YOY %") >= lower_bound_y) & (col("Transaction YOY %") <= upper_bound_y))

data.show(5)

from pyspark.ml.feature import MinMaxScaler, VectorAssembler

cols_to_normalize = ['Sales Index', 'Transactional Index', 'Sales MOM %',
                     'Sales YOY %', 'Transaction MOM %', 'Transaction YOY %']

for col in cols_to_normalize:
    assembler = VectorAssembler(inputCols=[col], outputCol=f"{col}_vec")
    data = assembler.transform(data)

    scaler = MinMaxScaler(inputCol=f"{col}_vec", outputCol=f"{col}_normalized")
    scaler_model = scaler.fit(data)
    data = scaler_model.transform(data)

    data = data.drop(f"{col}_vec")

data_scale = data.drop(*cols_to_normalize)
data_scale.show(5)

from pyspark.ml.stat import Correlation
from pyspark.ml.feature import VectorAssembler
import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd

numerical_columns = ['Year','Month','Sales Index_normalized', 'Transactional Index_normalized',
                     'Sales MOM %_normalized', 'Sales YOY %_normalized',
                     'Transaction MOM %_normalized', 'Transaction YOY %_normalized']

assembler = VectorAssembler(inputCols=numerical_columns, outputCol="features")
data_features = assembler.transform(data_scale).select("features")

correlation_matrix = Correlation.corr(data_features, "features").head()[0]

correlation_array = correlation_matrix.toArray()
correlation_df = pd.DataFrame(correlation_array, columns=numerical_columns, index=numerical_columns)

plt.figure(figsize=(10, 7))
sns.heatmap(correlation_df, annot=True, cmap='coolwarm', fmt=".2f")
plt.title('Correlation Matrix of Normalized Numerical Features')
plt.show()

"""Algorithms/Visualisations:

"""

from pyspark.sql.functions import udf
from pyspark.sql.types import DoubleType
from pyspark.ml.linalg import DenseVector

def extract_scalar_from_vector(vector):
    if isinstance(vector, DenseVector):
        return float(vector[0])
    return None

extract_scalar_udf = udf(extract_scalar_from_vector, DoubleType())

data_scale = data_scale.withColumn('label', extract_scalar_udf(data_scale['Sales Index_normalized']))

data_scale = data_scale.drop('Sales Index_normalized')
data_scale.printSchema()
data_assembled=data_scale

"""1. Linear Regression:"""

from pyspark.ml.regression import LinearRegression
from pyspark.ml.feature import VectorAssembler
from pyspark.sql import functions as F
import time


feature_columns = ['Transactional Index_normalized', 'Year', 'Sales YOY %_normalized', 'Transaction YOY %_normalized']

data_assembled = data_assembled.withColumnRenamed('Sales Index_normalized', 'label')

data_assembled = data_assembled.drop('featureslR')

vector_assembler = VectorAssembler(inputCols=feature_columns, outputCol="featureslR")
data_assembled = vector_assembler.transform(data_assembled)

train_data, test_data = data_assembled.randomSplit([0.8, 0.2], seed=42)

lr = LinearRegression(featuresCol='featureslR', labelCol='label')

start_time = time.time()
lr_model = lr.fit(train_data)
end_time = time.time()

predictions = lr_model.transform(test_data)

from pyspark.ml.evaluation import RegressionEvaluator
evaluator_rmse = RegressionEvaluator(labelCol='label', predictionCol='prediction', metricName='rmse')
evaluator_r2 = RegressionEvaluator(labelCol='label', predictionCol='prediction', metricName='r2')

rmse = evaluator_rmse.evaluate(predictions)
r2 = evaluator_r2.evaluate(predictions)

print(f"Linear Regression RMSE: {rmse:.4f}")
print(f"Linear Regression RÂ²: {r2:.4f}")
print(f"Execution Time: {end_time - start_time:.2f} seconds")

import matplotlib.pyplot as plt

predictions_pd = predictions.select('label', 'prediction').toPandas()

plt.figure(figsize=(8,6))

plt.scatter(predictions_pd['label'], predictions_pd['prediction'], color='blue', label='Predicted vs Actual')

plt.plot([predictions_pd['label'].min(), predictions_pd['label'].max()],
         [predictions_pd['label'].min(), predictions_pd['label'].max()],
         'r--', lw=2, label='Perfect Fit Line')

plt.xlabel('Actual Sales Index')
plt.ylabel('Predicted Sales Index')
plt.title('Linear Regression')

plt.legend()

plt.show()

"""2. Decision Tree Regression:"""

from pyspark.ml import Pipeline
from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler

data_assembled = data_assembled.drop('features')
data_assembled = data_assembled.drop('featureslR')

categorical_columns = ['State', 'Sector Name', 'Sub-Sector Name']
numeric_columns = [
    'Year', 'Month', 'Transactional Index_normalized', 'Sales MOM %_normalized',
    'Sales YOY %_normalized', 'Transaction YOY %_normalized'
]

indexers = [StringIndexer(inputCol=col, outputCol=col + "_index") for col in categorical_columns]
encoders = [OneHotEncoder(inputCol=col + "_index", outputCol=col + "_encoded") for col in categorical_columns]

encoded_columns = [col + "_encoded" for col in categorical_columns]
assembler = VectorAssembler(
    inputCols=encoded_columns + numeric_columns,
    outputCol='features'
)

pipeline = Pipeline(stages=indexers + encoders + [assembler])
data_encoded = pipeline.fit(data_assembled).transform(data_assembled)

columns_to_drop = categorical_columns + [col + "_index" for col in categorical_columns]
data_encoded = data_encoded.drop(*columns_to_drop)

data_encoded.show(truncate=False)

from pyspark.ml.regression import DecisionTreeRegressor
from pyspark.ml.evaluation import RegressionEvaluator
import time

train_data, test_data = data_encoded.randomSplit([0.8, 0.2], seed=42)

dt_regressor = DecisionTreeRegressor(featuresCol='features', labelCol='label')

start_time = time.time()
dt_model = dt_regressor.fit(train_data)
end_time = time.time()

predictions = dt_model.transform(test_data)

evaluator_rmse = RegressionEvaluator(labelCol='label', predictionCol='prediction', metricName='rmse')
evaluator_r2 = RegressionEvaluator(labelCol='label', predictionCol='prediction', metricName='r2')

rmse = evaluator_rmse.evaluate(predictions)
r2 = evaluator_r2.evaluate(predictions)

print(f"Decision Tree Regression RMSE: {rmse:.4f}")
print(f"Decision Tree Regression RÂ²: {r2:.4f}")
print(f"Execution Time: {end_time - start_time:.2f} seconds")

import matplotlib.pyplot as plt

predictions_pd = predictions.select('label', 'prediction').toPandas()

plt.figure(figsize=(8, 6))
plt.scatter(predictions_pd['label'], predictions_pd['prediction'], color='orange', alpha=0.6, label='Predicted vs. Actual')
plt.plot([predictions_pd['label'].min(), predictions_pd['label'].max()],
         [predictions_pd['label'].min(), predictions_pd['label'].max()], 'r--', lw=2, label='Perfect Fit Line')

plt.xlabel('Actual Sales Index')
plt.ylabel('Predicted Sales Index')
plt.title('Decision Tree Regression: Actual vs. Predicted')
plt.legend()
plt.grid(True)
plt.show()

"""3. Random Forest Regression:"""

from pyspark.ml.regression import RandomForestRegressor
from pyspark.ml.evaluation import RegressionEvaluator
import time

train_data, test_data = data_encoded.randomSplit([0.8, 0.2], seed=42)

rf_regressor = RandomForestRegressor(featuresCol='features', labelCol='label', numTrees=50, maxDepth=10, seed=42)

start_time = time.time()
rf_model = rf_regressor.fit(train_data)
end_time = time.time()

predictions = rf_model.transform(test_data)

evaluator_rmse = RegressionEvaluator(labelCol='label', predictionCol='prediction', metricName='rmse')
evaluator_r2 = RegressionEvaluator(labelCol='label', predictionCol='prediction', metricName='r2')

rmse = evaluator_rmse.evaluate(predictions)
r2 = evaluator_r2.evaluate(predictions)

print(f"Random Forest Regression RMSE: {rmse:.4f}")
print(f"Random Forest Regression RÂ²: {r2:.4f}")
print(f"Execution Time: {end_time - start_time:.2f} seconds")

import matplotlib.pyplot as plt
import pandas as pd

predictions_df = predictions.select("label", "prediction").toPandas()

plt.figure(figsize=(8, 6))
plt.scatter(predictions_df["label"], predictions_df["prediction"], color='red', alpha=0.6, label='Predicted vs. Actual')
plt.plot([predictions_df["label"].min(), predictions_df["label"].max()],
         [predictions_df["label"].min(), predictions_df["label"].max()], 'g--', lw=2, label='Perfect Fit Line')

plt.xlabel('Actual Sales Index')
plt.ylabel('Predicted Sales Index')
plt.title('Random Forest Regression')
plt.legend()
plt.show()

"""4. Gradient Boosting Regression:"""

from pyspark.ml.regression import GBTRegressor
from pyspark.ml.evaluation import RegressionEvaluator
import time

train_data, test_data = data_encoded.randomSplit([0.8, 0.2], seed=42)

gbt_regressor = GBTRegressor(featuresCol='features', labelCol='label', maxIter=50, maxDepth=5, seed=42)

start_time = time.time()
gbt_model = gbt_regressor.fit(train_data)
end_time = time.time()

predictions = gbt_model.transform(test_data)

evaluator_rmse = RegressionEvaluator(labelCol='label', predictionCol='prediction', metricName='rmse')
evaluator_r2 = RegressionEvaluator(labelCol='label', predictionCol='prediction', metricName='r2')

rmse = evaluator_rmse.evaluate(predictions)
r2 = evaluator_r2.evaluate(predictions)

print(f"Gradient Boosting Regression RMSE: {rmse:.4f}")
print(f"Gradient Boosting Regression RÂ²: {r2:.4f}")
print(f"Execution Time: {end_time - start_time:.2f} seconds")

import matplotlib.pyplot as plt

predictions_df = predictions.select("label", "prediction").toPandas()

plt.figure(figsize=(8, 6))
plt.scatter(predictions_df["label"], predictions_df["prediction"], color='yellow', alpha=0.6, label='Predicted vs. Actual')
plt.plot([predictions_df["label"].min(), predictions_df["label"].max()],
         [predictions_df["label"].min(), predictions_df["label"].max()], 'r--', lw=2, label='Perfect Fit Line')

plt.xlabel('Actual Sales Index')
plt.ylabel('Predicted Sales Index')
plt.title('Gradient Boosting Regression')
plt.legend()
plt.show()

"""5. XGBoost Regression:"""

pip install xgboost pyspark

from xgboost.spark import SparkXGBRegressor
from pyspark.ml.evaluation import RegressionEvaluator
import time
from pyspark.sql import SparkSession

train_data, test_data = data_encoded.randomSplit([0.8, 0.2], seed=42)

xgb_regressor = SparkXGBRegressor(features_col='features', label_col='label', num_workers=1,
                                  max_depth=5, num_round=100, learning_rate=0.1, seed=42)


start_time = time.time()
xgb_model = xgb_regressor.fit(train_data)
end_time = time.time()

predictions = xgb_model.transform(test_data)

evaluator_rmse = RegressionEvaluator(labelCol='label', predictionCol='prediction', metricName='rmse')
evaluator_r2 = RegressionEvaluator(labelCol='label', predictionCol='prediction', metricName='r2')

rmse = evaluator_rmse.evaluate(predictions)
r2 = evaluator_r2.evaluate(predictions)

print(f"XGBoost Regression RMSE: {rmse:.4f}")
print(f"XGBoost Regression RÂ²: {r2:.4f}")
print(f"Execution Time: {end_time - start_time:.2f} seconds")

import matplotlib.pyplot as plt

predictions_df = predictions.select("label", "prediction").toPandas()

plt.figure(figsize=(8, 6))
plt.scatter(predictions_df["label"], predictions_df["prediction"], color='purple', alpha=0.6, label='Predicted vs. Actual')
plt.plot([predictions_df["label"].min(), predictions_df["label"].max()],
         [predictions_df["label"].min(), predictions_df["label"].max()], 'r--', lw=2, label='Perfect Fit Line')

plt.xlabel('Actual Sales Index')
plt.ylabel('Predicted Sales Index')
plt.title('XGBoost Regression')
plt.legend()
plt.show()

"""6. Ridge Regression:"""

from pyspark.ml.regression import LinearRegression
from pyspark.ml.evaluation import RegressionEvaluator
import time

ridge_regressor = LinearRegression(featuresCol='features',
                                   labelCol='label',
                                   elasticNetParam=0.0)

start_time = time.time()
ridge_model = ridge_regressor.fit(train_data)
end_time = time.time()

predictions = ridge_model.transform(test_data)

evaluator_rmse = RegressionEvaluator(labelCol="label",
                                     predictionCol="prediction",
                                     metricName="rmse")
evaluator_r2 = RegressionEvaluator(labelCol="label",
                                   predictionCol="prediction",
                                   metricName="r2")

rmse = evaluator_rmse.evaluate(predictions)
r2 = evaluator_r2.evaluate(predictions)

print(f"Ridge Regression RMSE: {rmse:.4f}")
print(f"Ridge Regression RÂ²: {r2:.4f}")
print(f"Execution Time: {end_time - start_time:.2f} seconds")

import matplotlib.pyplot as plt

predictions_df = predictions.select("label", "prediction").toPandas()

plt.figure(figsize=(8, 6))
plt.scatter(predictions_df["label"], predictions_df["prediction"], color='seagreen', alpha=0.6, label='Predicted vs. Actual')
plt.plot([predictions_df["label"].min(), predictions_df["label"].max()],
         [predictions_df["label"].min(), predictions_df["label"].max()], 'r--', lw=2, label='Perfect Fit Line')

plt.xlabel('Actual Sales Index')
plt.ylabel('Predicted Sales Index')
plt.title('Ridge Regression')
plt.legend()
plt.show()

!pip install -q pyngrok
from pyngrok import ngrok
!ngrok config add-authtoken 2ofnyCIMhZLWxbK10LmBaP4dY6S_NfoQsbr2ZR6FsK3GRyh
public_url = ngrok.connect(4040)
print("Spark UI:",public_url)

